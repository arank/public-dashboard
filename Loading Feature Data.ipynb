{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Staging Crypto Orderbook Features for Training With IV Labs\n",
    "\n",
    "This is a brief example notebook demonstrating how to add and index new data sets from your local machine using the IV Labs client.\n",
    "\n",
    "First let's import the appropriate libraries and check if we have any feature servers up and running to try adding data to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arankhanna/anaconda/lib/python2.7/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['af605267f3a1111e8b3210ab26c3cd17-1311831915.us-east-1.elb.amazonaws.com']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arankhanna/anaconda/lib/python2.7/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from feature_server.historical.task import get_running_feature_server_urls\n",
    "\n",
    "# Let's print out the feature servers that are running\n",
    "feature_server_urls = get_running_feature_server_urls()\n",
    "print feature_server_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new server if necessary\n",
    "If there is no feature server running (if the command above returns no valid server addresses), we will run a feature server task then wait for the feature server to setup and get assigned an ip (This step might take some time (particularly IP assignment if you happen to be using azure), you can check the status of the external-ip assignment via kubectl get services and the status of the server itself via kubectl get pods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is no feature server URL listed above spin one up using the commented command\n",
    "from feature_server.historical.task import FeatureServerTask\n",
    "f = FeatureServerTask()\n",
    "f.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a client to verify connection\n",
    "Finally, once the feature server is given a url you can create a client feature server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_server.historical.client import Client\n",
    "# Once we can see a feature server running in the URL list let's conenct to it\n",
    "feature_server_client = Client(feature_server_urls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our feature server running and our client attached, we can register a new dataset on the server.\n",
    "\n",
    "To do this we need to give the data set a unqiue identifier (that muct match the sql specification for table name e.g. no dashes, special charecters etc.) as well as a fixed schema for the columns in the data set (in line with the standard [SQL Schema](https://www.postgresql.org/docs/9.5/static/datatype.html)), and finally we need to give the order of the columns to the feature server.\n",
    "\n",
    "For now only structured relational data can be registered and indexed by the feature server but we hope to change that soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_headers = ['time','sign','dp','c_csm_3','j_csm_3','h_csm_3','s_csm_3_dummy_5','s_csm_3_dummy_6','s_csm_3_dummy_7',\n",
    " 's_csm_3_dummy_10','s_csm_3_dummy_14','s_csm_3_dummy_15','s_csm_3_dummy_16','s_csm_3_dummy_17','s_csm_3_dummy_18',\n",
    " 's_csm_3_dummy_19','s_csm_3_dummy_20','s_csm_3_dummy_21','s_csm_3_dummy_22','s_csm_3_dummy_23','s_csm_3_dummy_24',\n",
    " 's_csm_3_dummy_25','s_csm_3_dummy_26','s_csm_3_dummy_27','s_csm_3_dummy_28','s_csm_3_dummy_29','s_csm_3_dummy_30',\n",
    " 's_csm_3_dummy_31','s_csm_3_dummy_32','sign_2','sign_3','sign_4','sign_5','sign_6','sign_7','sign_8','sign_9',\n",
    " 'sign_10','dp_lag_2','dp_lag_3','dp_lag_4','dp_lag_5','dp_lag_6','dp_lag_7','dp_lag_8','dp_lag_9','dp_lag_10',\n",
    " 'j_csm_3_lag_2','c_csm_3_lag_2','h_csm_3_lag_2','j_csm_3_lag_3','c_csm_3_lag_3','h_csm_3_lag_3','j_csm_3_lag_4',\n",
    " 'c_csm_3_lag_4','h_csm_3_lag_4','j_csm_3_lag_5','c_csm_3_lag_5','h_csm_3_lag_5','j_csm_3_lag_6','c_csm_3_lag_6',\n",
    " 'h_csm_3_lag_6','j_csm_3_lag_7','c_csm_3_lag_7','h_csm_3_lag_7','j_csm_3_lag_8','c_csm_3_lag_8','h_csm_3_lag_8',\n",
    " 'j_csm_3_lag_9','c_csm_3_lag_9','h_csm_3_lag_9','j_csm_3_lag_10','c_csm_3_lag_10','h_csm_3_lag_10',\n",
    " 'sign_-1_L_10_count','sign_-1_L_10_repetition','sign_0_L_10_count','sign_0_L_10_repetition','sign_1_L_10_count',\n",
    " 'sign_1_L_10_repetition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_types = {x: 'double precision' for x in feature_headers}\n",
    "feature_types['sign'] = 'int'\n",
    "feature_types['time'] = 'varchar(255)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_server_client.register_dataset('signprocess',  \n",
    "                                       feature_types,\n",
    "                                       feature_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have registered the data set we can stream data from a local file to append to the data set in the feature server using the `add_to_dataset` method. For now we only support csv files. When adding data to the feature server make sure the columns of these files are in the same order you indicated when creating the feature set above. Furthermore be sure to correctly indicate weather or not the csv file contains a header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'success': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from common.file_system.file_system import fs\n",
    "\n",
    "files = fs.list_files('features/v0/time_bars_3s', delimiter='')\n",
    "\n",
    "for f in files:\n",
    "    fs.download_file(f, 'dump/')\n",
    "    name = f.split('/')[-1]\n",
    "    feature_server_client.add_to_dataset('signprocess', file_path, file_type='csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you run this the feature data should also be available as flat files in the dump/ directory that was generated in the same folder as this notebook. If you want to model on this data outside of IV Lab, you can download it directly from this interface to Jupyter hub.\n",
    "\n",
    "It's important to remember all new data is appended to the existing dataset on the feature server when using the `add_to_dataset` method.\n",
    "\n",
    "Now that we have created the data set on the feature server and added some custom data, let's ensure everything has been indexed and loaded correctly by printing the number of rows indexed (should be 4 if you just loaded example.csv) as well as the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u't', u'integer'], [u'name', u'character varying'], [u'age', u'integer'], [u'gpa', u'double precision']]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Let's check the dataset schema\n",
    "print feature_server_client.query_for_dataset_schema('signprocess')\n",
    "\n",
    "# Let's see how many rows are in the data sets\n",
    "print feature_server_client.query_for_dataset_size('signprocess')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can keep adding from other files with the same column structure to this dataset or start loading it into arbitrary jobs running from IV Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
